# -*- coding: utf-8 -*-
"""2431363_saranashakya_classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrwValF3S7ppl7zi4N6rlqXXm2R8mskN
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
file_path = '/content/drive/MyDrive/AssessmentData/Waste_Management_and_Recycling_India.csv'
df = pd.read_csv(file_path)

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
if df.select_dtypes(include=['object']).shape[1] > 0:
    df = pd.get_dummies(df, drop_first=True)

# Define target column for classification (Example: Convert a numerical column into categories)

# classification_target = 'Waste Classification'  # Replace with actual classification column

# Instead of using the original column name, use one of the generated columns after one-hot encoding

# For example, if 'Waste Classification' had a category 'Organic', use 'Waste Classification_Organic'
classification_target = 'Waste Type_Organic'  # Replace with the actual column name from your dataset
if classification_target not in df.columns:
    raise KeyError(f"Column '{classification_target}' not found in dataset. Available columns: {df.columns.tolist()}")

# Encode target variable
le = LabelEncoder()
df[classification_target] = le.fit_transform(df[classification_target])

X = df.drop(columns=[classification_target])
y = df[classification_target]

# Distribution of target variable
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8, 5))
sns.countplot(x=classification_target, data=df, palette='coolwarm')
plt.title("Distribution of Target Variable")
plt.show()

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Classification Model: Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test data
y_pred = rf_classifier.predict(X_test) # This line is added to get predictions

# Evaluate Model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Final Classification Model: Random Forest Classifier")
print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Convert categorical variables to numeric using one-hot encoding
if df.select_dtypes(include=['object']).shape[1] > 0:
    df = pd.get_dummies(df, drop_first=True)

# Define target column for classification

# Choose one of the columns generated after one-hot encoding
classification_target = 'Waste Type_Organic'  # Replace with the actual desired column name

# Check if the chosen column exists
if classification_target not in df.columns:
    raise KeyError(f"Column '{classification_target}' not found in dataset. Available columns: {df.columns.tolist()}")

# Encode target variable
le = LabelEncoder()
df[classification_target] = le.fit_transform(df[classification_target])

X = df.drop(columns=[classification_target])
y = df[classification_target]

# Classification Model: Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

# Evaluate Model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Final Classification Model: Random Forest Classifier")
print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

# Exploratory Data Analysis (EDA)

# Summary statistics
print("\nStatistical Summary:")
print(df.describe())

# Import necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'file_path' is the path to your CSV file
file_path = '/content/drive/MyDrive/AssessmentData/Waste_Management_and_Recycling_India.csv'
df = pd.read_csv(file_path) # Load the dataframe here

# Pairplot
sns.pairplot(df, diag_kind='kde')
plt.show()

# Feature distributions
df.hist(figsize=(12, 10), bins=20, edgecolor= 'red' )
plt.suptitle("Feature Distributions")
plt.show()

# Boxplot for outlier detection
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, orient='h', palette='Set2')
plt.title("Boxplot for Outlier Detection")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Define target column for classification (Example: Convert a numerical column into categories)

# classification_target = 'Waste Classification'  # Replace with actual classification column

# Instead, use one of the existing columns from your dataset

# For example, let's use 'Waste Type' for classification
classification_target = 'Waste Type'

# Check if the chosen column exists
if classification_target not in df.columns:
    raise KeyError(f"Column '{classification_target}' not found in dataset. Available columns: {df.columns.tolist()}")

# Encode target variable
le = LabelEncoder()
df[classification_target] = le.fit_transform(df[classification_target])

# Convert categorical variables to numeric using one-hot encoding
if df.select_dtypes(include=['object']).shape[1] > 0:
    df = pd.get_dummies(df, drop_first=True)

X = df.drop(columns=[classification_target]).values
y = df[classification_target].values

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression from Scratch
class LogisticRegressionScratch:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.epochs):
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)

            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self.sigmoid(linear_model)
        return [1 if i > 0.5 else 0 for i in y_predicted]

# Train Logistic Regression Model
log_reg = LogisticRegressionScratch(learning_rate=0.01, epochs=1000)
log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)

# Evaluate Model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Final Classification Model: Logistic Regression from Scratch")
print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestClassifier

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Define target column for regression
regression_target = 'Waste Generated (Tons/Day)'  # Changed 'day' to 'Day' to match the actual column name
if regression_target not in df.columns:
    raise KeyError(f"Column '{regression_target}' not found in dataset. Available columns: {df.columns.tolist()}")

# Convert categorical variables to numeric using one-hot encoding
if df.select_dtypes(include=['object']).shape[1] > 0:
    df = pd.get_dummies(df, drop_first=True)

X = df.drop(columns=[regression_target]).values
y = df[regression_target].values

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
# Model 1: Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)
lin_mse = mean_squared_error(y_test, y_pred_lin)
lin_r2 = r2_score(y_test, y_pred_lin)

# Model 2: Random Forest classifier
rf_reg = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42) # Corrected the typo: 'classifier' to 'Classifier'
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)
rf_mse = mean_squared_error(y_test, y_pred_rf)
rf_r2 = r2_score(y_test, y_pred_rf)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Selecting relevant features
features = [
    "Waste Generated (Tons/Day)", "Recycling Rate (%)", "Population Density (People/km²)",
    "Municipal Efficiency Score (1-10)", "Cost of Waste Management (₹/Ton)", "Awareness Campaigns Count",
    "Landfill Capacity (Tons)", "Year"
]

# Target variable: Choose an existing column
# Check if the desired target column exists in the DataFrame
target_column = "Waste Type"  # Replace with your desired target column
if target_column not in df.columns:
    # Handle the case where the target column is not found
    # You can either raise an error, skip this step, or try to find a similar column
    raise KeyError(f"The column '{target_column}' is not found in the DataFrame. Available columns: {df.columns.tolist()}")
else:
    df = df[features + [target_column]] # Select the features and the target column

# Selecting relevant features
features = [
    "Waste Generated (Tons/Day)", "Recycling Rate (%)", "Population Density (People/km²)",
    "Municipal Efficiency Score (1-10)", "Cost of Waste Management (₹/Ton)", "Awareness Campaigns Count",
    "Landfill Capacity (Tons)", "Year",
    "Disposal Method" # Include 'Disposal Method' in the list of features
]

# Standardizing numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model 1: Logistic Regression
log_reg = LogisticRegression()
param_grid_log = {
    "C": [0.01, 0.1, 1, 10, 100],
    "solver": ["liblinear", "lbfgs"]
}
# Use 'cv=3' instead of 'cv=5' in GridSearchCV to address the error
grid_log = GridSearchCV(log_reg, param_grid_log, cv=3, scoring='accuracy')
grid_log.fit(X_train, y_train)

# Model 2: Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10]
}
grid_rf = GridSearchCV(rf_clf, param_grid_rf, cv=3, scoring='accuracy')  # Changed cv to 3
grid_rf.fit(X_train, y_train)

# Best parameters and cross-validation scores
print("Best parameters for Logistic Regression:", grid_log.best_params_)
print("Best cross-validation score for Logistic Regression:", grid_log.best_score_)

print("Best parameters for Random Forest:", grid_rf.best_params_)
print("Best cross-validation score for Random Forest:", grid_rf.best_score_)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE

# Selecting relevant features
features = [
    "Waste Generated (Tons/Day)", "Recycling Rate (%)", "Population Density (People/km²)",
    "Municipal Efficiency Score (1-10)", "Cost of Waste Management (₹/Ton)", "Awareness Campaigns Count",
    "Landfill Capacity (Tons)", "Year"
]

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Target variable: Disposal Method
target_column = "Disposal Method"  # Check the actual name

# Check if target column exists
if target_column not in df.columns:
    print(f"Error: '{target_column}' column not found in the dataset.")
    print(f"Available columns: {df.columns.tolist()}")
else:
    df = df[features + [target_column]]  # Select the features and target column

# Selecting relevant features
features = [
    "Waste Generated (Tons/Day)", "Recycling Rate (%)", "Population Density (People/km²)",
    "Municipal Efficiency Score (1-10)", "Cost of Waste Management (₹/Ton)", "Awareness Campaigns Count",
    "Landfill Capacity (Tons)", "Year",
    "Disposal Method"  # Add "Disposal Method" to the features list
]

# Standardizing numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model 1: Logistic Regression
log_reg = LogisticRegression()
param_grid_log = {
    "C": [0.01, 0.1, 1, 10, 100],
    "solver": ["liblinear", "lbfgs"]
}
grid_log = GridSearchCV(log_reg, param_grid_log, cv=3, scoring='accuracy')  # Changed cv to 3
grid_log.fit(X_train, y_train) # Remove the extra indentation here

# Feature Selection for Logistic Regression
rfe_log = RFE(grid_log.best_estimator_, n_features_to_select=5)
rfe_log.fit(X_train, y_train)

# Get the column names from the original DataFrame (df) before it was transformed.
# Assuming 'features' list contains the names of the columns used in X
selected_features_log = [features[i] for i in range(len(features)) if rfe_log.support_[i]]

print("Selected Features for Logistic Regression:", selected_features_log)

# Model 2: Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10]
}
# Change cv to a lower value (e.g., 3 or the minimum number of samples in any class)
grid_rf = GridSearchCV(rf_clf, param_grid_rf, cv=3, scoring='accuracy')
grid_rf.fit(X_train, y_train)

# Feature Selection for Random Forest
rfe_rf = RFE(grid_rf.best_estimator_, n_features_to_select=5)
rfe_rf.fit(X_train, y_train)
selected_features_rf = [features[i] for i in range(len(features)) if rfe_rf.support_[i]]  # Similar to Logistic Regression

# Best parameters and cross-validation scores
print("Best parameters for Logistic Regression:", grid_log.best_params_)
print("Best cross-validation score for Logistic Regression:", grid_log.best_score_)
print("Selected features for Logistic Regression:", list(selected_features_log))

print("Best parameters for Random Forest:", grid_rf.best_params_)
print("Best cross-validation score for Random Forest:", grid_rf.best_score_)
print("Selected features for Random Forest:", list(selected_features_rf))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score

# Selecting relevant features
features = [
    "Waste Generated (Tons/Day)", "Recycling Rate (%)", "Population Density (People/km²)",
    "Municipal Efficiency Score (1-10)", "Cost of Waste Management (₹/Ton)", "Awareness Campaigns Count",
    "Landfill Capacity (Tons)", "Year"
]

# Splitting dataset
# Check if 'Disposal Method' column exists in the DataFrame before dropping it
if "Disposal Method" in df.columns:
    X = df.drop(columns=["Disposal Method"])
    y = df["Disposal Method"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
else:
    print(f"Error: 'Disposal Method' column not found in the DataFrame. Available columns: {df.columns.tolist()}")
    # Handle the missing column: Either raise an exception, skip the operation, or try to find a similar column.
    # For example, you could raise an exception:
    # raise KeyError(f"'Disposal Method' column not found in the DataFrame.")

# Model 1: Logistic Regression
log_reg = LogisticRegression()
param_grid_log = {
    "C": [0.01, 0.1, 1, 10, 100],
    "solver": ["liblinear", "lbfgs"]
}
# Change cv to a lower value (e.g., 3 or the minimum number of samples in any class)
grid_log = GridSearchCV(log_reg, param_grid_log, cv=3, scoring='accuracy')
grid_log.fit(X_train, y_train)

# Splitting dataset
# Check if 'Disposal Method' column exists in the DataFrame before dropping it
if "Disposal Method" in df.columns:
    X = df.drop(columns=["Disposal Method"])  # Keep X as a DataFrame
    y = df["Disposal Method"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
else:
    print(f"Error: 'Disposal Method' column not found in the DataFrame. Available columns: {df.columns.tolist()}")
    # Handle the missing column: Either raise an exception, skip the operation, or try to find a similar column.
    # For example, you could raise an exception:
    # raise KeyError(f"'Disposal Method' column not found in the DataFrame.")

# Model 2: Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_split": [2, 5, 10]
}

# Change cv to a lower value (e.g., 3 or the minimum number of samples in any class) to ensure it's less than the smallest class size
grid_rf = GridSearchCV(rf_clf, param_grid_rf, cv=3, scoring='accuracy')
grid_rf.fit(X_train, y_train)
# Feature Selection for Random Forest
rfe_rf = RFE(grid_rf.best_estimator_, n_features_to_select=5)
rfe_rf.fit(X_train, y_train)

# Determine Best Model
best_model = grid_rf if grid_rf.best_score_ > grid_log.best_score_ else grid_log
best_features = selected_features_rf if best_model == grid_rf else selected_features_log

# Rebuild Final Model with Best Features
# Get column indices from the original DataFrame before it was converted to a NumPy array
# Assuming 'df' is your original DataFrame
# Ensure 'best_features' contains the actual names of your features, not just indices
# For example, instead of [0, 2, 4], 'best_features' should be ['feature1', 'feature3', 'feature5']

# Check if df is still a DataFrame, if not, you'll need to reload or recreate it
if isinstance(df, pd.DataFrame):
    best_feature_indices = [df.columns.get_loc(f) for f in best_features if f in df.columns]
    X_train_best = X_train[:, best_feature_indices]
    X_test_best = X_test[:, best_feature_indices]
else:
    print("Error: 'df' is not a DataFrame. Please reload or recreate it.")
    # Handle the error appropriately, e.g., by raising an exception or skipping the operation

final_model = best_model.best_estimator_
final_model.fit(X_train_best, y_train)

# Evaluate Final Model
y_pred = final_model.predict(X_test_best)
final_accuracy = accuracy_score(y_test, y_pred)

# Best parameters and results
print("Best Model:", "Random Forest" if best_model == grid_rf else "Logistic Regression")
print("Best Hyperparameters:", best_model.best_params_)
print("Selected Features:", list(best_features))
print("Final Model Accuracy:", final_accuracy)