# -*- coding: utf-8 -*-
"""2431363_SaranaShakya_Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-E4GBNS-gVQSZKIGJcTVaC_m7seU9VQ7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/AssessmentData/Waste_Management_and_Recycling_India.csv")

# Display basic information
print("Dataset Info:")
df.info()
print("\nFirst 5 rows:")
print(df.head())

# Handle missing values
print("\nMissing values before handling:")
print(df.isnull().sum())
df = df.dropna()  # Drop rows with missing values
print("\nMissing values after handling:")
print(df.isnull().sum())

# Handle duplicates
df = df.drop_duplicates()
print("\nDataset shape after removing duplicates:", df.shape)

# Convert data types if necessary (modify as needed)

# Example: df['column_name'] = pd.to_numeric(df['column_name'], errors='coerce')

# Summary statistics
print("\nSummary Statistics:")
print(df.describe())

# Train-test split
# Exploratory Data Analysis (EDA)
sns.set_style("whitegrid")

# Plot distribution of numerical columns
numerical_cols = df.select_dtypes(include=['number']).columns
for col in numerical_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.show()

# Pairplot for numerical variables
sns.pairplot(df[numerical_cols])
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 6))
numeric_df = df.select_dtypes(include=['number'])  # Select only numeric columns
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Categorical variable count plots
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(y=df[col], order=df[col].value_counts().index)
    plt.title(f"Count of {col}")
    plt.show()

# Outlier Detection and Removal using IQR
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
print("\nDataset shape after outlier removal:", df.shape)

# Encoding categorical variables
df = pd.get_dummies(df, drop_first=True)
print("\nDataset after encoding categorical variables:")
print(df.head())

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Linear Regression from Scratch
class LinearRegressionScratch:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.epochs):
            y_pred = np.dot(X, self.weights) + self.bias
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

# Prepare data for regression
target_column_name = 'Waste Generated (Tons/Day)'
if target_column_name not in df.columns:
    raise KeyError(f"Column '{target_column_name}' not found in dataset. Available columns: {df.columns.tolist()}")
X = df.drop(columns=[target_column_name])
y = df[target_column_name]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ensure target variable is numeric
y = pd.to_numeric(y, errors='coerce')

# Check for constant target values
if y.nunique() == 1:
    raise ValueError("Target variable has only one unique value. Cannot perform regression.")

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to numpy arrays
X_train, X_test = X_train.values, X_test.values
y_train, y_test = y_train.values, y_test.values

# Train linear regression model
lr = LinearRegressionScratch(learning_rate=0.01, epochs=1000)
# Convert X_train and y_train to numeric data types
# X_train_numeric = X_train.select_dtypes(include=np.number).astype(np.float64) # This line caused the error
X_train_numeric = X_train.astype(np.float64)  # Directly convert to float64
y_train_numeric = y_train.astype(np.float64)
lr.fit(X_train_numeric, y_train_numeric) # Removed .values since X_train_numeric is already a NumPy array

# Predict and evaluate
# Convert X_test to a DataFrame to use select_dtypes
X_test_df = pd.DataFrame(X_test)  # Create a DataFrame from X_test

# Predict and evaluate
# Convert X_test to a DataFrame to use select_dtypes
X_test_df = pd.DataFrame(X_test)  # Create a DataFrame from X_test
# %%
# Now you can use select_dtypes on the DataFrame
# Instead of selecting only numeric types, include all columns or investigate why no columns are numeric
# X_test_numeric = X_test_df.select_dtypes(include=np.number).astype(np.float64)
X_test_numeric = X_test_df.astype(np.float64)  # Include all columns

# Alternatively, investigate why no columns are numeric and adjust your preprocessing accordingly

y_pred = lr.predict(X_test_numeric.values)  # Predict using lr model

# Calculate metrics (make sure indentation is correct)
mse = np.mean((y_test - y_pred) ** 2)
r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

# Prepare data for regression
target_column_name = 'Waste Generated (Tons/Day)'  # Changed to match the actual column name
if target_column_name not in df.columns:
    raise KeyError(f"Column '{target_column_name}' not found in dataset. Available columns: {df.columns.tolist()}")

X = df.drop(columns=[target_column_name])
y = df[target_column_name]

# Ensure target variable is numeric
y = pd.to_numeric(y, errors='coerce')

# Check for constant target values
if y.nunique() == 1:
    raise ValueError("Target variable has only one unique value. Cannot perform regression.")

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Linear Regression
model_1 = LinearRegression()
model_1.fit(X_train, y_train)
y_pred_1 = model_1.predict(X_test)

mse_1 = mean_squared_error(y_test, y_pred_1)
r2_1 = r2_score(y_test, y_pred_1)

# Model 2: Random Forest Regressor
model_2 = RandomForestRegressor(n_estimators=100, random_state=42)
model_2.fit(X_train, y_train)
y_pred_2 = model_2.predict(X_test)

mse_2 = mean_squared_error(y_test, y_pred_2)
r2_2 = r2_score(y_test, y_pred_2)

# Compare results
print("Model 1: Linear Regression")
print(f"Mean Squared Error: {mse_1}")
print(f"R² Score: {r2_1}\n")

print("Model 2: Random Forest Regressor")
print(f"Mean Squared Error: {mse_2}")
print(f"R² Score: {r2_2}\n")

# Determine the better model
better_model = "Model 1: Linear Regression" if r2_1 > r2_2 else "Model 2: Random Forest Regressor"
print(f"Better Performing Model: {better_model}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

# Prepare data for regression
target_column_name = 'Waste Generated (Tons/Day)' # Changed 'day' to 'Day' to match the actual column name
if target_column_name not in df.columns:
    raise KeyError(f"Column '{target_column_name}' not found in dataset. Available columns: {df.columns.tolist()}")

X = df.drop(columns=[target_column_name])
y = df[target_column_name]

# Ensure target variable is numeric
y = pd.to_numeric(y, errors='coerce')

# Check for constant target values
if y.nunique() == 1:
    raise ValueError("Target variable has only one unique value. Cannot perform regression.")

# Split dataset# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Ridge Regression with Hyperparameter Tuning
ridge = Ridge()
param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}
ridge_cv = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='r2')
ridge_cv.fit(X_train, y_train)

best_ridge = ridge_cv.best_estimator_
y_pred_ridge = best_ridge.predict(X_test)

mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Model 2: Random Forest Regressor with Hyperparameter Tuning
rf = RandomForestRegressor(random_state=42)
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
rf_cv = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2', n_jobs=-1)
rf_cv.fit(X_train, y_train)

best_rf = rf_cv.best_estimator_
y_pred_rf = best_rf.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Compare results
print("Model 1: Ridge Regression")
print(f"Best Parameters: {ridge_cv.best_params_}")
print(f"Mean Squared Error: {mse_ridge}")
print(f"R² Score: {r2_ridge}\n")

print("Model 2: Random Forest Regressor")
print(f"Best Parameters: {rf_cv.best_params_}")
print(f"Mean Squared Error: {mse_rf}")
print(f"R² Score: {r2_rf}\n")

# Determine the better model
better_model = "Model 1: Ridge Regression" if r2_ridge > r2_rf else "Model 2: Random Forest Regressor"
print(f"Better Performing Model: {better_model}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import SelectFromModel

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

# Prepare data for regression
target_column_name = 'Waste Generated (Tons/Day)' # Changed 'day' to 'Day' to match the actual column name
if target_column_name not in df.columns:
    raise KeyError(f"Column '{target_column_name}' not found in dataset. Available columns: {df.columns.tolist()}")

X = df.drop(columns=[target_column_name])
y = df[target_column_name]

# Ensure target variable is numeric
y = pd.to_numeric(y, errors='coerce')

# Check for constant target values
if y.nunique() == 1:
    raise ValueError("Target variable has only one unique value. Cannot perform regression.")

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Selection for Model 1: Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
selector_ridge = SelectFromModel(ridge, prefit=True)
X_train_ridge = selector_ridge.transform(X_train)
X_test_ridge = selector_ridge.transform(X_test)
selected_features_ridge = df.drop(columns=[target_column_name]).columns[selector_ridge.get_support()]

# Feature Selection for Model 2: Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
selector_rf = SelectFromModel(rf, prefit=True)
X_train_rf = selector_rf.transform(X_train)
X_test_rf = selector_rf.transform(X_test)
selected_features_rf = df.drop(columns=[target_column_name]).columns[selector_rf.get_support()]

# Model 1: Ridge Regression with Selected Features
ridge.fit(X_train_ridge, y_train)
y_pred_ridge = ridge.predict(X_test_ridge)

mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Model 2: Random Forest Regressor with Selected Features
rf.fit(X_train_rf, y_train)
y_pred_rf = rf.predict(X_test_rf)

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Compare results
print("Model 1: Ridge Regression with Feature Selection")
print(f"Selected Features: {list(selected_features_ridge)}")
print(f"Mean Squared Error: {mse_ridge}")
print(f"R² Score: {r2_ridge}\n")

print("Model 2: Random Forest Regressor with Feature Selection")
print(f"Selected Features: {list(selected_features_rf)}")
print(f"Mean Squared Error: {mse_rf}")
print(f"R² Score: {r2_rf}\n")

# Determine the better model
better_model = "Model 1: Ridge Regression" if r2_ridge > r2_rf else "Model 2: Random Forest Regressor"
print(f"Better Performing Model: {better_model}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Ensure column names are stripped of whitespace
df.columns = df.columns.str.strip()

# Handle missing values
df = df.dropna()

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

# Prepare data for regression
target_column_name = 'Waste Generated (Tons/Day)'  # Changed 'day' to 'Day' to match the actual column name
if target_column_name not in df.columns:
    raise KeyError(f"Column '{target_column_name}' not found in dataset. Available columns: {df.columns.tolist()}")

X = df.drop(columns=[target_column_name])
y = df[target_column_name]

# Ensure target variable is numeric
y = pd.to_numeric(y, errors='coerce')

# Check for constant target values
if y.nunique() == 1:
    raise ValueError("Target variable has only one unique value. Cannot perform regression.")

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Best Model: Random Forest Regressor with Best Parameters and Features
# best_features = ['Feature1', 'Feature2', 'Feature3']  # Replace with actual selected features

# Assuming 'selected_features_rf' from previous cell holds the important features
best_features = selected_features_rf

selected_columns = [col for col in df.drop(columns=[target_column_name]).columns if col in best_features]
X_selected = df[selected_columns]
X_selected = scaler.fit_transform(X_selected)

X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

best_rf = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=5, random_state=42)
best_rf.fit(X_train, y_train)

y_pred_best = best_rf.predict(X_test)

mse_best = mean_squared_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

# Compare results
print("Final Model: Optimized Random Forest Regressor")
print(f"Selected Features: {best_features}")
print(f"Mean Squared Error: {mse_best}")
print(f"R² Score: {r2_best}")